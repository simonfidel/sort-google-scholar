{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M36VFanosbkb"
      },
      "source": [
        "# Sort Google Scholar\n",
        "This is a jupyter envirnment where you can try the code of the repository without installing anything. The only limitation is the robot checking problem which would require selenium and manual solution of the captchas, but for trying a few keywords, it should work!\n",
        "\n",
        "> **INSTRUCTIONS:** If this is the first time you are using a jupyter environment, you simply have to run the code blocks using the keyword `SHIFT` + `ENTER`. Make sure to update the keyword parameters when required.\n",
        "\n",
        "SortGS has been recently included to PyPI, so the instructions here got simpler. First, let's install the package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPot8aWcsfei",
        "outputId": "1c974ec7-b93e-4c12-9b96-d4b69c395924"
      },
      "outputs": [],
      "source": [
        "import sortgs\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import logging\n",
        "import fitz\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example `search_query`:\n",
        "\n",
        "- `Large Language Models` → General search\n",
        "- `\"Large Language Models\"` → Exact phrase search\n",
        "- `Large Language Models -transformer` → Exclude specific term\n",
        "- `Large Language Models author:\"Geoffrey Hinton\"` → Search by author\n",
        "- `Large Language Models source:Nature` → Search within a specific publication\n",
        "- `(\"Large Language Models\" OR \"Transformer Models\") AND (GPT OR BERT)` → Boolean search\n",
        "- `intitle:\"Large Language Models\"` → Search in the title only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dohZcNUmrost",
        "outputId": "359db4f7-7bd0-4eea-f4e0-efe6731c2b96"
      },
      "outputs": [],
      "source": [
        "# Main query\n",
        "search_query = 'airports'\n",
        "\n",
        "# Expanded form with extra parameters\n",
        "sortby = \"cit/year\"  # @param [\"Citations\", \"cit/year\"] {type:\"string\"}\n",
        "nresults = 100  # @param {type:\"number\"}\n",
        "startyear = '2000'  # @param {type:\"string\"}\n",
        "endyear = None  # @param {type:\"string\"}\n",
        "langfilter = None  # @param [\"None\", \"zh-CN\", \"zh-TW\", \"nl\", \"en\", \"fr\", \"de\", \"it\", \"ja\", \"ko\", \"pl\", \"pt\", \"es\", \"tr\"] {type:\"string\"}\n",
        "\n",
        "# Convert the langfilter to a list if it's not None\n",
        "if langfilter and langfilter != \"None\":\n",
        "    langfilter = [langfilter]\n",
        "else:\n",
        "    langfilter = None  # No language filter applied if \"None\" is selected\n",
        "\n",
        "# Constructing the base command\n",
        "cmd = f\"sortgs '{search_query}' --sortby '{sortby}' --nresults {nresults}\"\n",
        "\n",
        "if startyear:\n",
        "    cmd += f\" --startyear {startyear}\"\n",
        "\n",
        "if endyear:\n",
        "    cmd += f\" --endyear {endyear}\"\n",
        "\n",
        "if langfilter:\n",
        "    lang_str = ' '.join(langfilter)\n",
        "    cmd += f\" --langfilter {lang_str}\"\n",
        "\n",
        "# Output the constructed command for review\n",
        "print(\"Constructed command:\", cmd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_nuxpy_s_9c"
      },
      "source": [
        "> _**NOTE:** It is normal to get some warnings, for example year not found or author not found. However, if you get the robot checking warning, then it might not work anymore in the IP that you have. You can try going in 'Runtime' > 'Disconnect and delete runtime' to get a new IP. If the problem persists, then you will have to run locally using selenium and solve the captchas manually. Make sure to avoid running this code too often to avoid the robot checking problem._\n",
        "\n",
        "Next, you will see that a csv file with the name of the keyword was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "S1NzeUq0sOu-",
        "outputId": "cba23086-7481-4992-f7f8-a1163e2e2b4b"
      },
      "outputs": [],
      "source": [
        "csv_filename = search_query.replace(' ', '_')+'.csv'\n",
        "df = pd.read_csv(csv_filename)\n",
        "pd.set_option('display.max_colwidth', None)  # Set to None for full width\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Set up Selenium WebDriver (Headless Chrome)\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Headers for requests\n",
        "HEADERS = {\"User-Agent\": \"AbstractFetcher/1.0\"}\n",
        "\n",
        "# =================== FETCH ABSTRACTS FROM SEMANTIC SCHOLAR ===================\n",
        "def fetch_abstract_semantic_scholar(row):\n",
        "    \"\"\"Fetches the abstract using Semantic Scholar API, trying DOI first, then title.\"\"\"\n",
        "    \n",
        "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
        "    fields = \"title,abstract\"\n",
        "    headers = {\"User-Agent\": \"Academic-Abstract-Fetcher/1.0\"}\n",
        "\n",
        "    logging.info(\"Attempting to fetch abstract from Semantic Scholar for title: %s\", row['Title'])\n",
        "\n",
        "    # Attempt DOI-based retrieval\n",
        "    if '10.' in row['Source']:\n",
        "        doi = row['Source'].split('10.')[1].split('?')[0].split('/pdf')[0]\n",
        "        doi = \"10.\" + doi\n",
        "        doi_url = f\"{base_url}DOI:{doi}?fields={fields}\"\n",
        "\n",
        "        res = requests.get(doi_url, headers=headers)\n",
        "        \n",
        "        if res.status_code == 429:\n",
        "            logging.warning(\"Semantic Scholar rate limit hit. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "            return fetch_abstract_semantic_scholar(row)  # Retry once\n",
        "\n",
        "        if res.status_code == 200:\n",
        "            abstract = res.json().get('abstract')\n",
        "            if abstract:\n",
        "                logging.info(\"Abstract found via Semantic Scholar (DOI).\")\n",
        "                return abstract\n",
        "        else:\n",
        "            logging.debug(\"Semantic Scholar (DOI) returned status code: %d\", res.status_code)\n",
        "\n",
        "    # Fallback to title-based retrieval\n",
        "    title_query = requests.utils.quote(row['Title'])\n",
        "    title_search_url = f\"{base_url}search?query={title_query}&limit=1&fields={fields}\"\n",
        "\n",
        "    res = requests.get(title_search_url, headers=headers)\n",
        "\n",
        "    if res.status_code == 429:\n",
        "        logging.warning(\"Semantic Scholar rate limit hit. Retrying in 5 seconds...\")\n",
        "        time.sleep(5)\n",
        "        return fetch_abstract_semantic_scholar(row)  # Retry once\n",
        "\n",
        "    if res.status_code == 200:\n",
        "        data = res.json().get('data', [])\n",
        "        if data and 'abstract' in data[0]:\n",
        "            logging.info(\"Abstract found via Semantic Scholar (Title Search).\")\n",
        "            return data[0]['abstract']\n",
        "        else:\n",
        "            logging.warning(\"Title search returned no abstract.\")\n",
        "    else:\n",
        "        logging.debug(\"Semantic Scholar (Title) returned status code: %d\", res.status_code)\n",
        "\n",
        "    logging.info(\"Semantic Scholar did not return an abstract.\")\n",
        "    return None \n",
        "\n",
        "# =================== FETCH ABSTRACTS FROM CROSSREF ===================\n",
        "def fetch_crossref(doi):\n",
        "    \"\"\"Fetch abstract from CrossRef using DOI.\"\"\"\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if \"message\" in data and \"abstract\" in data[\"message\"]:\n",
        "                return data[\"message\"][\"abstract\"]\n",
        "        else:\n",
        "            logging.info(\"CrossRef did not return an abstract.\")\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"CrossRef request error: {e}\")\n",
        "\n",
        "    return None  # Return None if CrossRef fails\n",
        "\n",
        "# =================== FETCH ABSTRACTS VIA WEB SCRAPING ===================\n",
        "def fetch_abstract_web_scraping(url):\n",
        "    \"\"\"Attempts to fetch an abstract from a webpage using web scraping.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        \n",
        "        # Check if content type is a PDF\n",
        "        if \"application/pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "            logging.info(\"Skipping PDF content, trying PDF extraction...\")\n",
        "            return extract_abstract_from_pdf(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            abstract_candidates = soup.find_all([\"p\", \"div\"], string=True, limit=10)\n",
        "            \n",
        "            for tag in abstract_candidates:\n",
        "                text = tag.get_text(strip=True)\n",
        "                if len(text) > 100:  # Ensure a meaningful abstract\n",
        "                    return text\n",
        "        \n",
        "        logging.info(\"Web scraping did not return an abstract. Trying Selenium for JS-loaded pages...\")\n",
        "        return fetch_abstract_with_selenium(url)\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Web Scraping request error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# =================== FETCH ABSTRACTS USING SELENIUM ===================\n",
        "def fetch_abstract_with_selenium(url):\n",
        "    \"\"\"Uses Selenium to scrape JS-loaded abstracts.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        options = Options()\n",
        "        options.add_argument(\"--headless\")  # Run in headless mode\n",
        "        options.add_argument(\"--disable-gpu\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "        with webdriver.Chrome(options=options) as driver:\n",
        "            driver.get(url)\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "            \n",
        "            abstract_candidates = soup.find_all([\"p\", \"div\"], string=True, limit=10)\n",
        "            for tag in abstract_candidates:\n",
        "                text = tag.get_text(strip=True)\n",
        "                if len(text) > 100:\n",
        "                    return text\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Selenium Web Scraping failed: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# =================== FETCH ABSTRACTS FROM PDF ===================\n",
        "def extract_abstract_from_pdf(pdf_url):\n",
        "    \"\"\"Download and extract abstract from a PDF file.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(pdf_url, stream=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            with open(\"temp.pdf\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            with fitz.open(\"temp.pdf\") as doc:\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "\n",
        "                return text[:500]  # Limit to first 500 characters (likely abstract)\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"PDF extraction error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# =================== INTELLIGENT SOURCE SELECTION ===================\n",
        "def get_abstract(row):\n",
        "    \"\"\"Selects the best source for fetching abstracts based on available data.\"\"\"\n",
        "    \n",
        "    title = row.get(\"Title\", \"\").strip()\n",
        "    source_url = row.get(\"Source\", \"\").strip()\n",
        "    doi = row.get(\"DOI\", \"\").strip()\n",
        "\n",
        "    logging.info(f\"Starting abstract retrieval for: {title}\")\n",
        "\n",
        "    # 1. Try Semantic Scholar First (DOI-based, then Title-based)\n",
        "    logging.info(f\"Trying Semantic Scholar for Title: {title}\")\n",
        "    abstract = fetch_abstract_semantic_scholar(row)\n",
        "    if abstract:\n",
        "        return abstract\n",
        "\n",
        "    # 2. Try DOI-based lookup via CrossRef\n",
        "    if doi:\n",
        "        logging.info(f\"Trying CrossRef for DOI: {doi}\")\n",
        "        abstract = fetch_crossref(doi)\n",
        "        if abstract:\n",
        "            return abstract\n",
        "\n",
        "    # 3. Try Web Scraping if a valid source URL is available\n",
        "    if source_url.startswith(\"http\"):\n",
        "        logging.info(f\"Trying Web Scraping for Source: {source_url}\")\n",
        "        abstract = fetch_abstract_web_scraping(source_url)\n",
        "        if abstract:\n",
        "            return abstract\n",
        "\n",
        "    # 4. Try PDF Extraction if the source is a PDF\n",
        "    if source_url.endswith(\".pdf\"):\n",
        "        logging.info(f\"Trying PDF Extraction for: {source_url}\")\n",
        "        abstract = extract_abstract_from_pdf(source_url)\n",
        "        if abstract:\n",
        "            return abstract\n",
        "    \n",
        "    logging.warning(f\"Abstract not found for: {title}\")\n",
        "    return \"Abstract not found\"\n",
        "\n",
        "# =================== MAIN SCRIPT ===================\n",
        "# Fetch abstracts with a progress bar\n",
        "tqdm.pandas(desc=\"Fetching Abstracts\")\n",
        "# df['Abstract'] = df.progress_apply(get_abstract, axis=1)\n",
        "\n",
        "# Export enriched DataFrame to CSV\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(\"Abstract retrieval completed. Data saved to .csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Rank vs Citations\n",
        "view = df.reset_index().copy()\n",
        "\n",
        "# Function to truncate and add line breaks to long titles\n",
        "def shorten_title(title, max_length=60):\n",
        "    words = title.split()\n",
        "    shortened_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    # Add words to the current line until max_length is exceeded\n",
        "    for word in words:\n",
        "        if len(' '.join(current_line + [word])) <= max_length:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            shortened_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        shortened_lines.append(' '.join(current_line))\n",
        "\n",
        "    return '<br>'.join(shortened_lines)\n",
        "\n",
        "\n",
        "# Apply this function to the 'Title' column and create a new column for the shortened titles\n",
        "view['Short_Title'] = view['Title'].apply(shorten_title)\n",
        "\n",
        "# Now use 'Short_Title' for hover_name\n",
        "fig = px.scatter(view,\n",
        "                 x='Rank',\n",
        "                 y='Citations',\n",
        "                 title='Number of Citations vs Google Scholar Rank',\n",
        "                 hover_name='Short_Title',\n",
        "                 hover_data=['Rank', 'Author', 'Citations', 'Year', 'Publisher', 'Venue', 'cit/year']\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate .bib filename based on the CSV filename\n",
        "bib_filename = os.path.splitext(csv_filename)[0] + \".bib\"\n",
        "\n",
        "# Function to convert DataFrame to BibTeX\n",
        "def df_to_bib(df, filename):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            entry_type = \"article\"  # Assuming all are journal articles\n",
        "            citation_key = f\"{row['Author'].split(',')[0].split()[0]}{row['Year']}\"  # First author + year\n",
        "            entry = f\"\"\"@{entry_type}{{{citation_key},\n",
        "    author = {{{row['Author']}}},\n",
        "    title = {{{row['Title']}}},\n",
        "    year = {{{row['Year']}}},\n",
        "    publisher = {{{row['Publisher']}}},\n",
        "    journal = {{{row['Venue']}}},\n",
        "    url = {{{row['Source']}}}\n",
        "}}\\n\\n\"\"\"\n",
        "            f.write(entry)\n",
        "\n",
        "# Export DataFrame as .bib with the same name as the CSV\n",
        "df_to_bib(df.head(10), bib_filename)\n",
        "\n",
        "print(f\"BibTeX file exported successfully as {bib_filename}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Test sortgs.py on Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
